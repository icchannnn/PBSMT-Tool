nohup: ignoring input
Using SCRIPTS_ROOTDIR: /mnt/c/smt/smt/mosesdecoder/scripts
Using single-thread GIZA
using gzip 
(1) preparing corpus @ Fri Nov 10 15:11:20 PST 2023
Executing: mkdir -p /mnt/c/smt/smt/working/train/corpus
(1.0) selecting factors @ Fri Nov 10 15:11:20 PST 2023
(1.1) running mkcls  @ Fri Nov 10 15:11:20 PST 2023
/mnt/c/smt/smt/mosesdecoder/tools/mkcls -c50 -n2 -p/mnt/c/smt/smt/corpus/corpus.clean.fil -V/mnt/c/smt/smt/working/train/corpus/fil.vcb.classes opt
Executing: /mnt/c/smt/smt/mosesdecoder/tools/mkcls -c50 -n2 -p/mnt/c/smt/smt/corpus/corpus.clean.fil -V/mnt/c/smt/smt/working/train/corpus/fil.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 1181

start-costs: MEAN: 27133.8 (27102.4-27165.2)  SIGMA:31.3914   
  end-costs: MEAN: 22241.1 (22124.5-22357.6)  SIGMA:116.554   
   start-pp: MEAN: 78.1024 (77.4153-78.7896)  SIGMA:0.687131   
     end-pp: MEAN: 19.8308 (19.1833-20.4784)  SIGMA:0.647575   
 iterations: MEAN: 26901 (26878-26924)  SIGMA:23   
       time: MEAN: 0.179688 (0.140625-0.21875)  SIGMA:0.0390625   
(1.1) running mkcls  @ Fri Nov 10 15:11:20 PST 2023
/mnt/c/smt/smt/mosesdecoder/tools/mkcls -c50 -n2 -p/mnt/c/smt/smt/corpus/corpus.clean.eng -V/mnt/c/smt/smt/working/train/corpus/eng.vcb.classes opt
Executing: /mnt/c/smt/smt/mosesdecoder/tools/mkcls -c50 -n2 -p/mnt/c/smt/smt/corpus/corpus.clean.eng -V/mnt/c/smt/smt/working/train/corpus/eng.vcb.classes opt

***** 2 runs. (algorithm:TA)*****
;KategProblem:cats: 50   words: 1741

start-costs: MEAN: 52438.4 (52394.6-52482.1)  SIGMA:43.7439   
  end-costs: MEAN: 43441.6 (43394.1-43489.1)  SIGMA:47.4996   
   start-pp: MEAN: 212.769 (211.289-214.249)  SIGMA:1.47968   
     end-pp: MEAN: 50.901 (50.5167-51.2854)  SIGMA:0.384377   
 iterations: MEAN: 41255.5 (40240-42271)  SIGMA:1015.5   
       time: MEAN: 0.351562 (0.328125-0.375)  SIGMA:0.0234375   
(1.2) creating vcb file /mnt/c/smt/smt/working/train/corpus/fil.vcb @ Fri Nov 10 15:11:21 PST 2023
(1.2) creating vcb file /mnt/c/smt/smt/working/train/corpus/eng.vcb @ Fri Nov 10 15:11:21 PST 2023
(1.3) numberizing corpus /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt @ Fri Nov 10 15:11:21 PST 2023
(1.3) numberizing corpus /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt @ Fri Nov 10 15:11:21 PST 2023
(2) running giza @ Fri Nov 10 15:11:21 PST 2023
(2.1a) running snt2cooc fil-eng @ Fri Nov 10 15:11:21 PST 2023

Executing: mkdir -p /mnt/c/smt/smt/working/train/giza.fil-eng
Executing: /mnt/c/smt/smt/mosesdecoder/tools/snt2cooc.out /mnt/c/smt/smt/working/train/corpus/eng.vcb /mnt/c/smt/smt/working/train/corpus/fil.vcb /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt > /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.cooc
/mnt/c/smt/smt/mosesdecoder/tools/snt2cooc.out /mnt/c/smt/smt/working/train/corpus/eng.vcb /mnt/c/smt/smt/working/train/corpus/fil.vcb /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt > /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.cooc
END.
(2.1b) running giza fil-eng @ Fri Nov 10 15:11:21 PST 2023
/mnt/c/smt/smt/mosesdecoder/tools/GIZA++  -CoocurrenceFile /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.cooc -c /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng -onlyaldumps 1 -p0 0.999 -s /mnt/c/smt/smt/working/train/corpus/eng.vcb -t /mnt/c/smt/smt/working/train/corpus/fil.vcb
Executing: /mnt/c/smt/smt/mosesdecoder/tools/GIZA++  -CoocurrenceFile /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.cooc -c /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng -onlyaldumps 1 -p0 0.999 -s /mnt/c/smt/smt/working/train/corpus/eng.vcb -t /mnt/c/smt/smt/working/train/corpus/fil.vcb
/mnt/c/smt/smt/mosesdecoder/tools/GIZA++  -CoocurrenceFile /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.cooc -c /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng -onlyaldumps 1 -p0 0.999 -s /mnt/c/smt/smt/working/train/corpus/eng.vcb -t /mnt/c/smt/smt/working/train/corpus/fil.vcb
Parameter 'coocurrencefile' changed from '' to '/mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.cooc'
Parameter 'c' changed from '' to '/mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2023-11-10.151121.kryztiancagandahan' to '/mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/mnt/c/smt/smt/working/train/corpus/eng.vcb'
Parameter 't' changed from '' to '/mnt/c/smt/smt/working/train/corpus/fil.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2023-11-10.151121.kryztiancagandahan.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/smt/smt/working/train/corpus/eng.vcb  (source vocabulary file name)
t = /mnt/c/smt/smt/working/train/corpus/fil.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2023-11-10.151121.kryztiancagandahan.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/smt/smt/working/train/corpus/eng.vcb  (source vocabulary file name)
t = /mnt/c/smt/smt/working/train/corpus/fil.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/mnt/c/smt/smt/working/train/corpus/eng.vcb
Reading vocabulary file from:/mnt/c/smt/smt/working/train/corpus/fil.vcb
Source vocabulary list has 1742 unique tokens 
Target vocabulary list has 1182 unique tokens 
Calculating vocabulary frequencies from corpus /mnt/c/smt/smt/working/train/corpus/fil-eng-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 896 sentence pairs.
 Train total # sentence pairs (weighted): 896
Size of source portion of the training corpus: 5394 tokens
Size of the target portion of the training corpus: 2672 tokens 
In source portion of the training corpus, only 1741 unique tokens appeared
In target portion of the training corpus, only 1180 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 2672/(6290-896)== 0.495365
There are 13905 13905 entries in table
==========================================================
Model1 Training Started at: Fri Nov 10 15:11:21 2023

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 11.8923 PERPLEXITY 3801.35
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 14.5097 PERPLEXITY 23327.1
Model 1 Iteration: 1 took: 1 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 4.95475 PERPLEXITY 31.0119
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 6.20906 PERPLEXITY 73.98
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 4.73341 PERPLEXITY 26.6011
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 6.01484 PERPLEXITY 64.6616
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.6471 PERPLEXITY 25.0562
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.82492 PERPLEXITY 56.6859
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.60982 PERPLEXITY 24.417
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.71504 PERPLEXITY 52.5289
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 1 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 1741  #classes: 51
Read classes: #words: 1181  #classes: 51

==========================================================
Hmm Training Started at: Fri Nov 10 15:11:22 2023

-----------
Hmm: Iteration 1
A/D table contains 1111 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.59044 PERPLEXITY 24.0913
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.64581 PERPLEXITY 50.0678

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 1111 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.01812 PERPLEXITY 16.2022
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.73731 PERPLEXITY 26.6731

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 1111 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 3.87489 PERPLEXITY 14.6709
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.41226 PERPLEXITY 21.2924

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 1111 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 3.73854 PERPLEXITY 13.3479
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 4.16618 PERPLEXITY 17.9534

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 1111 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 3.61479 PERPLEXITY 12.2507
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 3.97067 PERPLEXITY 15.678

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 0 seconds
==========================================================
Read classes: #words: 1741  #classes: 51
Read classes: #words: 1181  #classes: 51
Read classes: #words: 1741  #classes: 51
Read classes: #words: 1181  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Fri Nov 10 15:11:22 2023


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 20.9297 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 1111 parameters.
A/D table contains 554 parameters.
NTable contains 17420 parameter.
p0_count is 2063.46 and p1 is 304.272; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 1.92639 PERPLEXITY 3.80104
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 2.13961 PERPLEXITY 4.40642

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 20.5815 #alsophisticatedcountcollection: 0 #hcsteps: 1.44754
#peggingImprovements: 0
A/D table contains 1111 parameters.
A/D table contains 546 parameters.
NTable contains 17420 parameter.
p0_count is 2638.6 and p1 is 16.6996; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.98956 PERPLEXITY 15.8847
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.18656 PERPLEXITY 18.2088

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 20.5703 #alsophisticatedcountcollection: 0 #hcsteps: 1.38839
#peggingImprovements: 0
A/D table contains 1111 parameters.
A/D table contains 546 parameters.
NTable contains 17420 parameter.
p0_count is 2663.74 and p1 is 4.13023; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 3.74863 PERPLEXITY 13.4415
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 3.92607 PERPLEXITY 15.2007

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 20.6094 #alsophisticatedcountcollection: 7.24442 #hcsteps: 1.43415
#peggingImprovements: 0
D4 table contains 245833 parameters.
A/D table contains 1111 parameters.
A/D table contains 546 parameters.
NTable contains 17420 parameter.
p0_count is 2666.36 and p1 is 2.81992; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 3.66144 PERPLEXITY 12.6533
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 3.8198 PERPLEXITY 14.1213

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 20.6384 #alsophisticatedcountcollection: 6.59933 #hcsteps: 1.45982
#peggingImprovements: 0
D4 table contains 245833 parameters.
A/D table contains 1111 parameters.
A/D table contains 538 parameters.
NTable contains 17420 parameter.
p0_count is 2665.91 and p1 is 3.04587; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.37876 PERPLEXITY 10.4018
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 3.52434 PERPLEXITY 11.5062

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 20.7109 #alsophisticatedcountcollection: 5.95089 #hcsteps: 1.51674
#peggingImprovements: 0
D4 table contains 245833 parameters.
A/D table contains 1111 parameters.
A/D table contains 538 parameters.
NTable contains 17420 parameter.
p0_count is 2666.03 and p1 is 2.98736; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 3.28683 PERPLEXITY 9.75963
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 3.42116 PERPLEXITY 10.7121

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Fri Nov 10 15:11:22 2023


Entire Viterbi H333444 Training took: 0 seconds
==========================================================

Entire Training took: 1 seconds
Program Finished at: Fri Nov 10 15:11:22 2023

==========================================================
Executing: rm -f /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.A3.final.gz
Executing: gzip /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.A3.final
(2.1a) running snt2cooc eng-fil @ Fri Nov 10 15:11:22 PST 2023

Executing: mkdir -p /mnt/c/smt/smt/working/train/giza.eng-fil
Executing: /mnt/c/smt/smt/mosesdecoder/tools/snt2cooc.out /mnt/c/smt/smt/working/train/corpus/fil.vcb /mnt/c/smt/smt/working/train/corpus/eng.vcb /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt > /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.cooc
/mnt/c/smt/smt/mosesdecoder/tools/snt2cooc.out /mnt/c/smt/smt/working/train/corpus/fil.vcb /mnt/c/smt/smt/working/train/corpus/eng.vcb /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt > /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.cooc
END.
(2.1b) running giza eng-fil @ Fri Nov 10 15:11:22 PST 2023
/mnt/c/smt/smt/mosesdecoder/tools/GIZA++  -CoocurrenceFile /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.cooc -c /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil -onlyaldumps 1 -p0 0.999 -s /mnt/c/smt/smt/working/train/corpus/fil.vcb -t /mnt/c/smt/smt/working/train/corpus/eng.vcb
Executing: /mnt/c/smt/smt/mosesdecoder/tools/GIZA++  -CoocurrenceFile /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.cooc -c /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil -onlyaldumps 1 -p0 0.999 -s /mnt/c/smt/smt/working/train/corpus/fil.vcb -t /mnt/c/smt/smt/working/train/corpus/eng.vcb
/mnt/c/smt/smt/mosesdecoder/tools/GIZA++  -CoocurrenceFile /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.cooc -c /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt -m1 5 -m2 0 -m3 3 -m4 3 -model1dumpfrequency 1 -model4smoothfactor 0.4 -nodumps 1 -nsmooth 4 -o /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil -onlyaldumps 1 -p0 0.999 -s /mnt/c/smt/smt/working/train/corpus/fil.vcb -t /mnt/c/smt/smt/working/train/corpus/eng.vcb
Parameter 'coocurrencefile' changed from '' to '/mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.cooc'
Parameter 'c' changed from '' to '/mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt'
Parameter 'm3' changed from '5' to '3'
Parameter 'm4' changed from '5' to '3'
Parameter 'model1dumpfrequency' changed from '0' to '1'
Parameter 'model4smoothfactor' changed from '0.2' to '0.4'
Parameter 'nodumps' changed from '0' to '1'
Parameter 'nsmooth' changed from '64' to '4'
Parameter 'o' changed from '2023-11-10.151122.kryztiancagandahan' to '/mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil'
Parameter 'onlyaldumps' changed from '0' to '1'
Parameter 'p0' changed from '-1' to '0.999'
Parameter 's' changed from '' to '/mnt/c/smt/smt/working/train/corpus/fil.vcb'
Parameter 't' changed from '' to '/mnt/c/smt/smt/working/train/corpus/eng.vcb'
general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2023-11-10.151122.kryztiancagandahan.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/smt/smt/working/train/corpus/fil.vcb  (source vocabulary file name)
t = /mnt/c/smt/smt/working/train/corpus/eng.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

general parameters:
-------------------
ml = 101  (maximum sentence length)

No. of iterations:
-------------------
hmmiterations = 5  (mh)
model1iterations = 5  (number of iterations for Model 1)
model2iterations = 0  (number of iterations for Model 2)
model3iterations = 3  (number of iterations for Model 3)
model4iterations = 3  (number of iterations for Model 4)
model5iterations = 0  (number of iterations for Model 5)
model6iterations = 0  (number of iterations for Model 6)

parameter for various heuristics in GIZA++ for efficient training:
------------------------------------------------------------------
countincreasecutoff = 1e-06  (Counts increment cutoff threshold)
countincreasecutoffal = 1e-05  (Counts increment cutoff threshold for alignments in training of fertility models)
mincountincrease = 1e-07  (minimal count increase)
peggedcutoff = 0.03  (relative cutoff probability for alignment-centers in pegging)
probcutoff = 1e-07  (Probability cutoff threshold for lexicon probabilities)
probsmooth = 1e-07  (probability smoothing (floor) value )

parameters for describing the type and amount of output:
-----------------------------------------------------------
compactalignmentformat = 0  (0: detailled alignment format, 1: compact alignment format )
hmmdumpfrequency = 0  (dump frequency of HMM)
l = 2023-11-10.151122.kryztiancagandahan.log  (log file name)
log = 0  (0: no logfile; 1: logfile)
model1dumpfrequency = 1  (dump frequency of Model 1)
model2dumpfrequency = 0  (dump frequency of Model 2)
model345dumpfrequency = 0  (dump frequency of Model 3/4/5)
nbestalignments = 0  (for printing the n best alignments)
nodumps = 1  (1: do not write any files)
o = /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil  (output file prefix)
onlyaldumps = 1  (1: do not write any files)
outputpath =   (output path)
transferdumpfrequency = 0  (output: dump of transfer from Model 2 to 3)
verbose = 0  (0: not verbose; 1: verbose)
verbosesentence = -10  (number of sentence for which a lot of information should be printed (negative: no output))

parameters describing input files:
----------------------------------
c = /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt  (training corpus file name)
d =   (dictionary file name)
s = /mnt/c/smt/smt/working/train/corpus/fil.vcb  (source vocabulary file name)
t = /mnt/c/smt/smt/working/train/corpus/eng.vcb  (target vocabulary file name)
tc =   (test corpus file name)

smoothing parameters:
---------------------
emalsmooth = 0.2  (f-b-trn: smoothing factor for HMM alignment model (can be ignored by -emSmoothHMM))
model23smoothfactor = 0  (smoothing parameter for IBM-2/3 (interpolation with constant))
model4smoothfactor = 0.4  (smooting parameter for alignment probabilities in Model 4)
model5smoothfactor = 0.1  (smooting parameter for distortion probabilities in Model 5 (linear interpolation with constant))
nsmooth = 4  (smoothing for fertility parameters (good value: 64): weight for wordlength-dependent fertility parameters)
nsmoothgeneral = 0  (smoothing for fertility parameters (default: 0): weight for word-independent fertility parameters)

parameters modifying the models:
--------------------------------
compactadtable = 1  (1: only 3-dimensional alignment table for IBM-2 and IBM-3)
deficientdistortionforemptyword = 0  (0: IBM-3/IBM-4 as described in (Brown et al. 1993); 1: distortion model of empty word is deficient; 2: distoriton model of empty word is deficient (differently); setting this parameter also helps to avoid that during IBM-3 and IBM-4 training too many words are aligned with the empty word)
depm4 = 76  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
depm5 = 68  (d_{=1}: &1:l, &2:m, &4:F, &8:E, d_{>1}&16:l, &32:m, &64:F, &128:E)
emalignmentdependencies = 2  (lextrain: dependencies in the HMM alignment model.  &1: sentence length; &2: previous class; &4: previous position;  &8: French position; &16: French class)
emprobforempty = 0.4  (f-b-trn: probability for empty word)

parameters modifying the EM-algorithm:
--------------------------------------
m5p0 = -1  (fixed value for parameter p_0 in IBM-5 (if negative then it is determined in training))
manlexfactor1 = 0  ()
manlexfactor2 = 0  ()
manlexmaxmultiplicity = 20  ()
maxfertility = 10  (maximal fertility for fertility models)
p0 = 0.999  (fixed value for parameter p_0 in IBM-3/4 (if negative then it is determined in training))
pegging = 0  (0: no pegging; 1: do pegging)

reading vocabulary files 
Reading vocabulary file from:/mnt/c/smt/smt/working/train/corpus/fil.vcb
Reading vocabulary file from:/mnt/c/smt/smt/working/train/corpus/eng.vcb
Source vocabulary list has 1182 unique tokens 
Target vocabulary list has 1742 unique tokens 
Calculating vocabulary frequencies from corpus /mnt/c/smt/smt/working/train/corpus/eng-fil-int-train.snt
Reading more sentence pairs into memory ... 
Corpus fits in memory, corpus has: 896 sentence pairs.
 Train total # sentence pairs (weighted): 896
Size of source portion of the training corpus: 2672 tokens
Size of the target portion of the training corpus: 5394 tokens 
In source portion of the training corpus, only 1181 unique tokens appeared
In target portion of the training corpus, only 1740 unique tokens appeared
lambda for PP calculation in IBM-1,IBM-2,HMM:= 5394/(3568-896)== 2.01871
There are 14465 14465 entries in table
==========================================================
Model1 Training Started at: Fri Nov 10 15:11:22 2023

-----------
Model1: Iteration 1
Model1: (1) TRAIN CROSS-ENTROPY 11.7411 PERPLEXITY 3423.09
Model1: (1) VITERBI TRAIN CROSS-ENTROPY 13.709 PERPLEXITY 13391
Model 1 Iteration: 1 took: 0 seconds
-----------
Model1: Iteration 2
Model1: (2) TRAIN CROSS-ENTROPY 5.22949 PERPLEXITY 37.5174
Model1: (2) VITERBI TRAIN CROSS-ENTROPY 5.96791 PERPLEXITY 62.592
Model 1 Iteration: 2 took: 0 seconds
-----------
Model1: Iteration 3
Model1: (3) TRAIN CROSS-ENTROPY 5.0316 PERPLEXITY 32.7086
Model1: (3) VITERBI TRAIN CROSS-ENTROPY 5.78549 PERPLEXITY 55.1578
Model 1 Iteration: 3 took: 0 seconds
-----------
Model1: Iteration 4
Model1: (4) TRAIN CROSS-ENTROPY 4.95762 PERPLEXITY 31.0737
Model1: (4) VITERBI TRAIN CROSS-ENTROPY 5.64893 PERPLEXITY 50.1763
Model 1 Iteration: 4 took: 0 seconds
-----------
Model1: Iteration 5
Model1: (5) TRAIN CROSS-ENTROPY 4.92604 PERPLEXITY 30.4008
Model1: (5) VITERBI TRAIN CROSS-ENTROPY 5.5693 PERPLEXITY 47.4816
Model 1 Iteration: 5 took: 0 seconds
Entire Model1 Training took: 0 seconds
NOTE: I am doing iterations with the HMM model!
Read classes: #words: 1181  #classes: 51
Read classes: #words: 1741  #classes: 51

==========================================================
Hmm Training Started at: Fri Nov 10 15:11:22 2023

-----------
Hmm: Iteration 1
A/D table contains 638 parameters.
Hmm: (1) TRAIN CROSS-ENTROPY 4.90954 PERPLEXITY 30.0552
Hmm: (1) VITERBI TRAIN CROSS-ENTROPY 5.51872 PERPLEXITY 45.8459

Hmm Iteration: 1 took: 0 seconds

-----------
Hmm: Iteration 2
A/D table contains 638 parameters.
Hmm: (2) TRAIN CROSS-ENTROPY 4.30792 PERPLEXITY 19.8067
Hmm: (2) VITERBI TRAIN CROSS-ENTROPY 4.66408 PERPLEXITY 25.3529

Hmm Iteration: 2 took: 0 seconds

-----------
Hmm: Iteration 3
A/D table contains 638 parameters.
Hmm: (3) TRAIN CROSS-ENTROPY 4.17939 PERPLEXITY 18.1184
Hmm: (3) VITERBI TRAIN CROSS-ENTROPY 4.44036 PERPLEXITY 21.7111

Hmm Iteration: 3 took: 0 seconds

-----------
Hmm: Iteration 4
A/D table contains 638 parameters.
Hmm: (4) TRAIN CROSS-ENTROPY 4.10121 PERPLEXITY 17.1627
Hmm: (4) VITERBI TRAIN CROSS-ENTROPY 4.31685 PERPLEXITY 19.9297

Hmm Iteration: 4 took: 0 seconds

-----------
Hmm: Iteration 5
A/D table contains 638 parameters.
Hmm: (5) TRAIN CROSS-ENTROPY 4.04866 PERPLEXITY 16.5489
Hmm: (5) VITERBI TRAIN CROSS-ENTROPY 4.23999 PERPLEXITY 18.8958

Hmm Iteration: 5 took: 0 seconds

Entire Hmm Training took: 0 seconds
==========================================================
Read classes: #words: 1181  #classes: 51
Read classes: #words: 1741  #classes: 51
Read classes: #words: 1181  #classes: 51
Read classes: #words: 1741  #classes: 51

==========================================================
Starting H333444:  Viterbi Training
 H333444 Training Started at: Fri Nov 10 15:11:22 2023


---------------------
THTo3: Iteration 1
#centers(pre/hillclimbed/real): 1 1 1  #al: 29.0156 #alsophisticatedcountcollection: 0 #hcsteps: 0
#peggingImprovements: 0
A/D table contains 638 parameters.
A/D table contains 1192 parameters.
NTable contains 11820 parameter.
p0_count is 4433.69 and p1 is 480.156; p0 is 0.999 p1: 0.001
THTo3: TRAIN CROSS-ENTROPY 3.26481 PERPLEXITY 9.6118
THTo3: (1) TRAIN VITERBI CROSS-ENTROPY 3.48381 PERPLEXITY 11.1875

THTo3 Viterbi Iteration : 1 took: 0 seconds

---------------------
Model3: Iteration 2
#centers(pre/hillclimbed/real): 1 1 1  #al: 28.5011 #alsophisticatedcountcollection: 0 #hcsteps: 1.38616
#peggingImprovements: 0
A/D table contains 638 parameters.
A/D table contains 1192 parameters.
NTable contains 11820 parameter.
p0_count is 4951.76 and p1 is 221.122; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.78055 PERPLEXITY 27.4846
Model3: (2) TRAIN VITERBI CROSS-ENTROPY 4.8632 PERPLEXITY 29.1051

Model3 Viterbi Iteration : 2 took: 0 seconds

---------------------
Model3: Iteration 3
#centers(pre/hillclimbed/real): 1 1 1  #al: 28.2879 #alsophisticatedcountcollection: 0 #hcsteps: 1.42969
#peggingImprovements: 0
A/D table contains 638 parameters.
A/D table contains 1181 parameters.
NTable contains 11820 parameter.
p0_count is 5162.81 and p1 is 115.596; p0 is 0.999 p1: 0.001
Model3: TRAIN CROSS-ENTROPY 4.57195 PERPLEXITY 23.7845
Model3: (3) TRAIN VITERBI CROSS-ENTROPY 4.6258 PERPLEXITY 24.689

Model3 Viterbi Iteration : 3 took: 0 seconds

---------------------
T3To4: Iteration 4
#centers(pre/hillclimbed/real): 1 1 1  #al: 28.1819 #alsophisticatedcountcollection: 6.16183 #hcsteps: 1.4375
#peggingImprovements: 0
D4 table contains 166257 parameters.
A/D table contains 638 parameters.
A/D table contains 1181 parameters.
NTable contains 11820 parameter.
p0_count is 5216.61 and p1 is 88.6956; p0 is 0.999 p1: 0.001
T3To4: TRAIN CROSS-ENTROPY 4.51064 PERPLEXITY 22.795
T3To4: (4) TRAIN VITERBI CROSS-ENTROPY 4.5522 PERPLEXITY 23.4612

T3To4 Viterbi Iteration : 4 took: 0 seconds

---------------------
Model4: Iteration 5
#centers(pre/hillclimbed/real): 1 1 1  #al: 28.0357 #alsophisticatedcountcollection: 4.49554 #hcsteps: 1.4375
#peggingImprovements: 0
D4 table contains 166257 parameters.
A/D table contains 638 parameters.
A/D table contains 1181 parameters.
NTable contains 11820 parameter.
p0_count is 5234.57 and p1 is 79.7168; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.22058 PERPLEXITY 18.6432
Model4: (5) TRAIN VITERBI CROSS-ENTROPY 4.25296 PERPLEXITY 19.0663

Model4 Viterbi Iteration : 5 took: 0 seconds

---------------------
Model4: Iteration 6
#centers(pre/hillclimbed/real): 1 1 1  #al: 27.8493 #alsophisticatedcountcollection: 3.83817 #hcsteps: 1.43862
#peggingImprovements: 0
D4 table contains 166257 parameters.
A/D table contains 638 parameters.
A/D table contains 1181 parameters.
NTable contains 11820 parameter.
p0_count is 5248.48 and p1 is 72.7609; p0 is 0.999 p1: 0.001
Model4: TRAIN CROSS-ENTROPY 4.16092 PERPLEXITY 17.888
Model4: (6) TRAIN VITERBI CROSS-ENTROPY 4.18689 PERPLEXITY 18.2129

Model4 Viterbi Iteration : 6 took: 0 seconds
H333444 Training Finished at: Fri Nov 10 15:11:22 2023


Entire Viterbi H333444 Training took: 0 seconds
==========================================================

Entire Training took: 0 seconds
Program Finished at: Fri Nov 10 15:11:22 2023

==========================================================
Executing: rm -f /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.A3.final.gz
Executing: gzip /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.A3.final
(3) generate word alignment @ Fri Nov 10 15:11:22 PST 2023
Combining forward and inverted alignment from files:
  /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.A3.final.{bz2,gz}
  /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.A3.final.{bz2,gz}
Executing: mkdir -p /mnt/c/smt/smt/working/train/model
Executing: /mnt/c/smt/smt/mosesdecoder/scripts/training/giza2bal.pl -d "gzip -cd /mnt/c/smt/smt/working/train/giza.eng-fil/eng-fil.A3.final.gz" -i "gzip -cd /mnt/c/smt/smt/working/train/giza.fil-eng/fil-eng.A3.final.gz" |/mnt/c/smt/smt/mosesdecoder/tools/bin/symal -alignment="grow" -diagonal="yes" -final="yes" -both="yes" > /mnt/c/smt/smt/working/train/model/aligned.grow-diag-final-and
symal: computing grow alignment: diagonal (1) final (1)both-uncovered (1)
skip=<0> counts=<896>
(4) generate lexical translation table 0-0 @ Fri Nov 10 15:11:22 PST 2023
(/mnt/c/smt/smt/corpus/corpus.clean.fil,/mnt/c/smt/smt/corpus/corpus.clean.eng,/mnt/c/smt/smt/working/train/model/lex)
!
Saved: /mnt/c/smt/smt/working/train/model/lex.f2e and /mnt/c/smt/smt/working/train/model/lex.e2f
FILE: /mnt/c/smt/smt/corpus/corpus.clean.eng
FILE: /mnt/c/smt/smt/corpus/corpus.clean.fil
FILE: /mnt/c/smt/smt/working/train/model/aligned.grow-diag-final-and
(5) extract phrases @ Fri Nov 10 15:11:22 PST 2023
/mnt/c/smt/smt/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /mnt/c/smt/smt/mosesdecoder/tools/bin/extract /mnt/c/smt/smt/corpus/corpus.clean.eng /mnt/c/smt/smt/corpus/corpus.clean.fil /mnt/c/smt/smt/working/train/model/aligned.grow-diag-final-and /mnt/c/smt/smt/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
Executing: /mnt/c/smt/smt/mosesdecoder/scripts/generic/extract-parallel.perl 4 split "sort    " /mnt/c/smt/smt/mosesdecoder/tools/bin/extract /mnt/c/smt/smt/corpus/corpus.clean.eng /mnt/c/smt/smt/corpus/corpus.clean.fil /mnt/c/smt/smt/working/train/model/aligned.grow-diag-final-and /mnt/c/smt/smt/working/train/model/extract 7 orientation --model wbe-msd --GZOutput 
MAX 7 1 0
Started Fri Nov 10 15:11:22 2023
using gzip 
isBSDSplit=0 
Executing: mkdir -p /mnt/c/smt/smt/working/train/model/tmp.583; ls -l /mnt/c/smt/smt/working/train/model/tmp.583 
total=896 line-per-split=225 
split -d -l 225 -a 7 /mnt/c/smt/smt/corpus/corpus.clean.eng /mnt/c/smt/smt/working/train/model/tmp.583/target.split -d -l 225 -a 7 /mnt/c/smt/smt/corpus/corpus.clean.fil /mnt/c/smt/smt/working/train/model/tmp.583/source.split -d -l 225 -a 7 /mnt/c/smt/smt/working/train/model/aligned.grow-diag-final-and /mnt/c/smt/smt/working/train/model/tmp.583/align.merging extract / extract.inv
gunzip -c /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000000.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000001.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000002.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000003.gz  | LC_ALL=C sort     -T /mnt/c/smt/smt/working/train/model/tmp.583 2>> /dev/stderr | gzip -c > /mnt/c/smt/smt/working/train/model/extract.sorted.gz 2>> /dev/stderr 
gunzip -c /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000000.inv.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000001.inv.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000002.inv.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000003.inv.gz  | LC_ALL=C sort     -T /mnt/c/smt/smt/working/train/model/tmp.583 2>> /dev/stderr | gzip -c > /mnt/c/smt/smt/working/train/model/extract.inv.sorted.gz 2>> /dev/stderr 
gunzip -c /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000000.o.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000001.o.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000002.o.gz /mnt/c/smt/smt/working/train/model/tmp.583/extract.0000003.o.gz  | LC_ALL=C sort     -T /mnt/c/smt/smt/working/train/model/tmp.583 2>> /dev/stderr | gzip -c > /mnt/c/smt/smt/working/train/model/extract.o.sorted.gz 2>> /dev/stderr 
Finished Fri Nov 10 15:11:23 2023
(6) score phrases @ Fri Nov 10 15:11:23 PST 2023
(6.1)  creating table half /mnt/c/smt/smt/working/train/model/phrase-table.half.f2e @ Fri Nov 10 15:11:23 PST 2023
/mnt/c/smt/smt/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /mnt/c/smt/smt/mosesdecoder/tools/bin/score /mnt/c/smt/smt/working/train/model/extract.sorted.gz /mnt/c/smt/smt/working/train/model/lex.f2e /mnt/c/smt/smt/working/train/model/phrase-table.half.f2e.gz  0 
Executing: /mnt/c/smt/smt/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /mnt/c/smt/smt/mosesdecoder/tools/bin/score /mnt/c/smt/smt/working/train/model/extract.sorted.gz /mnt/c/smt/smt/working/train/model/lex.f2e /mnt/c/smt/smt/working/train/model/phrase-table.half.f2e.gz  0 
using gzip 
Started Fri Nov 10 15:11:23 2023
/mnt/c/smt/smt/mosesdecoder/tools/bin/score /mnt/c/smt/smt/working/train/model/tmp.632/extract.0.gz /mnt/c/smt/smt/working/train/model/lex.f2e /mnt/c/smt/smt/working/train/model/tmp.632/phrase-table.half.0000000.gz  2>> /dev/stderr 
/mnt/c/smt/smt/working/train/model/tmp.632/run.0.sh/mnt/c/smt/smt/working/train/model/tmp.632/run.1.sh/mnt/c/smt/smt/working/train/model/tmp.632/run.2.sh/mnt/c/smt/smt/working/train/model/tmp.632/run.3.shmv /mnt/c/smt/smt/working/train/model/tmp.632/phrase-table.half.0000000.gz /mnt/c/smt/smt/working/train/model/phrase-table.half.f2e.gzrm -rf /mnt/c/smt/smt/working/train/model/tmp.632 
Finished Fri Nov 10 15:11:24 2023
(6.3)  creating table half /mnt/c/smt/smt/working/train/model/phrase-table.half.e2f @ Fri Nov 10 15:11:24 PST 2023
/mnt/c/smt/smt/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /mnt/c/smt/smt/mosesdecoder/tools/bin/score /mnt/c/smt/smt/working/train/model/extract.inv.sorted.gz /mnt/c/smt/smt/working/train/model/lex.e2f /mnt/c/smt/smt/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
Executing: /mnt/c/smt/smt/mosesdecoder/scripts/generic/score-parallel.perl 4 "sort    " /mnt/c/smt/smt/mosesdecoder/tools/bin/score /mnt/c/smt/smt/working/train/model/extract.inv.sorted.gz /mnt/c/smt/smt/working/train/model/lex.e2f /mnt/c/smt/smt/working/train/model/phrase-table.half.e2f.gz --Inverse 1 
using gzip 
Started Fri Nov 10 15:11:24 2023
/mnt/c/smt/smt/mosesdecoder/tools/bin/score /mnt/c/smt/smt/working/train/model/tmp.655/extract.0.gz /mnt/c/smt/smt/working/train/model/lex.e2f /mnt/c/smt/smt/working/train/model/tmp.655/phrase-table.half.0000000.gz --Inverse  2>> /dev/stderr 
/mnt/c/smt/smt/working/train/model/tmp.655/run.0.sh/mnt/c/smt/smt/working/train/model/tmp.655/run.1.sh/mnt/c/smt/smt/working/train/model/tmp.655/run.2.sh/mnt/c/smt/smt/working/train/model/tmp.655/run.3.shgunzip -c /mnt/c/smt/smt/working/train/model/tmp.655/phrase-table.half.*.gz 2>> /dev/stderr| LC_ALL=C sort     -T /mnt/c/smt/smt/working/train/model/tmp.655  | gzip -c > /mnt/c/smt/smt/working/train/model/phrase-table.half.e2f.gz  2>> /dev/stderr rm -rf /mnt/c/smt/smt/working/train/model/tmp.655 
Finished Fri Nov 10 15:11:24 2023
(6.6) consolidating the two halves @ Fri Nov 10 15:11:24 PST 2023
Executing: /mnt/c/smt/smt/mosesdecoder/tools/bin/consolidate /mnt/c/smt/smt/working/train/model/phrase-table.half.f2e.gz /mnt/c/smt/smt/working/train/model/phrase-table.half.e2f.gz /dev/stdout | gzip -c > /mnt/c/smt/smt/working/train/model/phrase-table.gz
Consolidate v2.0 written by Philipp Koehn
consolidating direct and indirect rule tables

Executing: rm -f /mnt/c/smt/smt/working/train/model/phrase-table.half.*
(7) learn reordering model @ Fri Nov 10 15:11:24 PST 2023
(7.1) [no factors] learn reordering model @ Fri Nov 10 15:11:24 PST 2023
(7.2) building tables @ Fri Nov 10 15:11:24 PST 2023
Executing: /mnt/c/smt/smt/mosesdecoder/tools/bin/lexical-reordering-score /mnt/c/smt/smt/working/train/model/extract.o.sorted.gz 0.5 /mnt/c/smt/smt/working/train/model/reordering-table. --model "wbe msd wbe-msd-bidirectional-fe"
Lexical Reordering Scorer
scores lexical reordering models of several types (hierarchical, phrase-based and word-based-extraction
(8) learn generation model @ Fri Nov 10 15:11:24 PST 2023
  no generation model requested, skipping step
(9) create moses.ini @ Fri Nov 10 15:11:24 PST 2023
